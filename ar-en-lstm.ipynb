{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport random\nfrom datasets import load_dataset\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport re\nfrom collections import defaultdict\nimport torch.nn.functional as F\nfrom nltk.translate.bleu_score import SmoothingFunction\n\n# Custom tokenizer functions\ndef simple_arabic_tokenizer(text):\n    # Basic Arabic tokenizer that splits on whitespace and punctuation\n    text = re.sub(r'[\\u0617-\\u061A\\u064B-\\u0652]', '', text)  # Replace punctuation with space\n    return text.split()\n\ndef simple_english_tokenizer(text):\n    # Basic English tokenizer\n    text=text.lower()\n    text = re.sub(r'[^\\w\\s]', ' ', text)  # Replace punctuation with space\n    return text.split()\n\n# Load dataset with 10,000 samples\ndataset = load_dataset(\"opus100\", \"ar-en\")\ntrain_data = dataset['train'].shuffle(seed=42).select(range(50000))\nvalid_data = dataset['validation'].shuffle(seed=42).select(range(2000))\ntest_data = dataset['test'].shuffle(seed=42).select(range(2000))\n\n# Build vocabulary\n# Modified build_vocab function with vocabulary size limit\ndef build_vocab(data, tokenizer_fn, language, max_vocab_size=10000):\n    vocab = defaultdict(int)\n    special_tokens = ['<pad>', '<unk>', '<sos>', '<eos>']\n    \n    for item in data:\n        tokens = tokenizer_fn(item['translation'][language])\n        for token in tokens:\n            vocab[token] += 1\n    \n    # Sort by frequency and limit vocabulary size\n    sorted_vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n    \n    # Take top (max_vocab_size - len(special_tokens)) most frequent tokens\n    sorted_vocab = sorted_vocab[:max_vocab_size - len(special_tokens)]\n    \n    word_to_idx = {word: idx+len(special_tokens) for idx, (word, count) in enumerate(sorted_vocab)}\n    \n    # Add special tokens\n    for idx, token in enumerate(special_tokens):\n        word_to_idx[token] = idx\n    \n    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n    \n    return word_to_idx, idx_to_word\n\n# Build vocabularies with size limit\nar_word_to_idx, ar_idx_to_word = build_vocab(train_data, simple_arabic_tokenizer, 'ar', max_vocab_size=10000)\nen_word_to_idx, en_idx_to_word = build_vocab(train_data, simple_english_tokenizer, 'en', max_vocab_size=10000)\n\nprint(f\"Arabic vocab size: {len(ar_word_to_idx)} (Limited to 10,000)\")\nprint(f\"English vocab size: {len(en_word_to_idx)} (Limited to 10,000)\")\n\n# Dataset class\nclass TranslationDataset(Dataset):\n    def __init__(self, data, max_length=50):\n        self.data = data\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]['translation']\n        arabic_text = item['ar']\n        english_text = item['en']\n\n        # Tokenize\n        arabic_tokens = simple_arabic_tokenizer(arabic_text)+['<eos>']\n        english_tokens = ['<sos>'] + simple_english_tokenizer(english_text) + ['<eos>']\n\n        # Convert to indices with unknown handling\n        arabic_indices = [ar_word_to_idx.get(token, ar_word_to_idx['<unk>']) for token in arabic_tokens]\n        english_indices = [en_word_to_idx.get(token, en_word_to_idx['<unk>']) for token in english_tokens]\n\n        # Pad or truncate\n        def pad_or_truncate(sequence, pad_idx):\n            if len(sequence) > self.max_length:\n                return sequence[:self.max_length]\n            return sequence + [pad_idx] * (self.max_length - len(sequence))\n\n        arabic_indices = pad_or_truncate(arabic_indices, ar_word_to_idx['<pad>'])\n        english_indices = pad_or_truncate(english_indices, en_word_to_idx['<pad>'])\n\n        return {\n            'arabic': torch.tensor(arabic_indices, dtype=torch.long),\n            'english': torch.tensor(english_indices, dtype=torch.long)\n        }\n\n# Create dataloaders\nBATCH_SIZE = 32\ntrain_dataset = TranslationDataset(train_data)\nvalid_dataset = TranslationDataset(valid_data)\ntest_dataset = TranslationDataset(test_data)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H3VlnBEpSUl3","outputId":"76c30f42-4c97-4c10-98d1-5c575c27a4f0","trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:15:23.441018Z","iopub.execute_input":"2025-05-15T12:15:23.441421Z","iopub.status.idle":"2025-05-15T12:15:33.449401Z","shell.execute_reply.started":"2025-05-15T12:15:23.441392Z","shell.execute_reply":"2025-05-15T12:15:33.448640Z"}},"outputs":[{"name":"stdout","text":"Arabic vocab size: 10000 (Limited to 10,000)\nEnglish vocab size: 10000 (Limited to 10,000)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"class BahdanauAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.W1 = nn.Linear(hidden_dim * 2, hidden_dim)  # For encoder outputs\n        self.W2 = nn.Linear(hidden_dim * 2, hidden_dim)  # For decoder hidden state\n        self.V = nn.Linear(hidden_dim, 1)               # For attention scores\n\n    def forward(self, decoder_hidden, encoder_outputs):\n        # decoder_hidden: (batch_size, hidden_dim*2)\n        # encoder_outputs: (seq_len, batch_size, hidden_dim*2)\n\n        seq_len, batch_size, hidden_dim = encoder_outputs.shape #(50,32,600)\n        \n        # Expand decoder hidden state to match encoder outputs\n        decoder_hidden = decoder_hidden.unsqueeze(0) #(32,600) -> (1,32,600)\n        \n        # Compute attention scores\n        energy = torch.tanh(self.W1(encoder_outputs) + self.W2(decoder_hidden))  # (seq_len, batch_size, hidden_dim)\n        attention_scores = self.V(energy).squeeze(2)  # (seq_len, batch_size)\n        \n        # Compute attention weights\n        attention_weights = F.softmax(attention_scores, dim=0)  # (seq_len, batch_size)\n\n        # Compute context vector as weighted sum of encoder outputs\n        context = torch.sum(encoder_outputs * attention_weights.unsqueeze(2), dim=0)  # (batch_size, hidden_dim*2)\n        \n        return context, attention_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:15:33.450757Z","iopub.execute_input":"2025-05-15T12:15:33.451065Z","iopub.status.idle":"2025-05-15T12:15:33.456671Z","shell.execute_reply.started":"2025-05-15T12:15:33.451023Z","shell.execute_reply":"2025-05-15T12:15:33.455929Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"class Seq2SeqWithAttention(nn.Module):\n    def __init__(self, input_dim, output_dim, emb_dim=256, hidden_dim=512, \n                 n_layers=2, dropout=0.3):\n        super().__init__()\n        \n        # Encoder (bidirectional)\n        self.encoder_embedding = nn.Embedding(input_dim, emb_dim, padding_idx=0)\n        self.encoder = nn.LSTM(emb_dim, hidden_dim, n_layers,\n                              dropout=dropout if n_layers > 1 else 0,\n                              bidirectional=True)\n        \n        # Decoder components\n        self.decoder_embedding = nn.Embedding(output_dim, emb_dim, padding_idx=0)\n        self.attention = BahdanauAttention(hidden_dim)\n        self.decoder = nn.LSTM(emb_dim + hidden_dim*2, hidden_dim*2, n_layers,\n                              dropout=dropout if n_layers > 1 else 0)\n        \n        # Output layers\n        self.fc_out = nn.Linear(hidden_dim*4, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size = trg.shape[1]\n        trg_len = trg.shape[0]\n        trg_vocab_size = self.fc_out.out_features\n        \n        # Initialize outputs\n        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(src.device)\n        \n        # Encoder forward pass\n        embedded = self.dropout(self.encoder_embedding(src))\n        encoder_outputs, (hidden, cell) = self.encoder(embedded)\n        \n        # Reshape encoder final states (bidirectional to unidirectional)\n        hidden = self._reshape_hidden(hidden)\n        cell = self._reshape_hidden(cell)\n        \n        # First decoder input is <sos> token\n        input = trg[0,:]\n        \n        for t in range(1, trg_len):\n            # Decoder forward pass with attention\n            embedded = self.dropout(self.decoder_embedding(input.unsqueeze(0)))\n            \n            # Calculate attention context\n            context, _ = self.attention(hidden[-1], encoder_outputs) # (32,600)  , (32,50,600)\n            context = context.unsqueeze(0)\n            # Combine embedded input with context\n            \n            rnn_input = torch.cat((embedded, context), dim=2)\n            \n            # Decoder step\n            output, (hidden, cell) = self.decoder(rnn_input, (hidden, cell))\n            \n            # Final prediction\n            output = torch.cat((output, context), dim=2)\n            pred = self.fc_out(output.squeeze(0))\n            outputs[t] = pred\n            \n            # Teacher forcing\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = pred.argmax(1)\n            input = trg[t] if teacher_force else top1\n        \n        return outputs\n    \n    def _reshape_hidden(self, hidden):\n        \"\"\"Convert bidirectional LSTM outputs to decoder-compatible format\"\"\"\n        hidden = hidden.view(self.encoder.num_layers, 2, -1, self.encoder.hidden_size)\n        return torch.cat((hidden[:, 0, :, :], hidden[:, 1, :, :]), dim=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:15:33.457470Z","iopub.execute_input":"2025-05-15T12:15:33.457679Z","iopub.status.idle":"2025-05-15T12:15:33.471627Z","shell.execute_reply.started":"2025-05-15T12:15:33.457663Z","shell.execute_reply":"2025-05-15T12:15:33.471035Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"\nINPUT_DIM = len(ar_word_to_idx)\nOUTPUT_DIM = len(en_word_to_idx)\nENC_EMB_DIM = 256 \nDEC_EMB_DIM = 256  \nHID_DIM = 300      \nN_LAYERS = 1      \nENC_DROPOUT = 0.7  \nDEC_DROPOUT = 0.7  \n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = Seq2SeqWithAttention(\n    INPUT_DIM, OUTPUT_DIM, ENC_EMB_DIM, HID_DIM,\n    N_LAYERS, ENC_DROPOUT\n).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)  \ncriterion = nn.CrossEntropyLoss(ignore_index=en_word_to_idx['<pad>'])\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nuhy9gWUzins","outputId":"9478089f-c084-4fbd-c62e-e1fb0c3d0ae0","trusted":true,"execution":{"iopub.status.busy":"2025-05-15T13:54:20.088828Z","iopub.execute_input":"2025-05-15T13:54:20.089458Z","iopub.status.idle":"2025-05-15T13:54:20.308247Z","shell.execute_reply.started":"2025-05-15T13:54:20.089434Z","shell.execute_reply":"2025-05-15T13:54:20.307493Z"}},"outputs":[{"name":"stdout","text":"The model has 22,329,301 trainable parameters\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"def train(model, loader, optimizer, criterion, clip):\n    model.train()\n    epoch_loss = 0\n\n    for batch in loader:\n        src = batch['arabic'].permute(1, 0).to(device)\n        trg = batch['english'].permute(1, 0).to(device)\n        \n        optimizer.zero_grad()\n        output = model(src, trg[:-1,:])\n\n        output_dim = output.shape[-1]\n        output = output.reshape(-1, output_dim)  # Changed view to reshape\n        trg = trg[1:].reshape(-1)\n\n        loss = criterion(output, trg)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    return epoch_loss / len(loader)\n\ndef evaluate(model, loader, criterion):\n    model.eval()\n    epoch_loss = 0\n\n    with torch.no_grad():\n        for batch in loader:\n            src = batch['arabic'].permute(1, 0).to(device)\n            trg = batch['english'].permute(1, 0).to(device)\n\n            output = model(src, trg[:-1,:])\n\n            output_dim = output.shape[-1]\n            output = output.reshape(-1, output_dim)  # Changed view to reshape\n            trg = trg[1:].reshape(-1)\n\n            loss = criterion(output, trg)\n            epoch_loss += loss.item()\n\n    return epoch_loss / len(loader)\n\nN_EPOCHS = 10  # Reduced training time\nCLIP = 1\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n    valid_loss = evaluate(model, valid_loader, criterion)\n\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'best_model_small.pt')\n\n    print(f'Epoch: {epoch+1:02}')\n    print(f'\\tTrain Loss: {train_loss:.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7RWQFtzNzk_i","outputId":"50808b80-34f9-4f0f-be0b-56667e8d5a12","trusted":true,"execution":{"iopub.status.busy":"2025-05-15T13:54:23.943939Z","iopub.execute_input":"2025-05-15T13:54:23.944527Z","iopub.status.idle":"2025-05-15T15:04:44.383590Z","shell.execute_reply.started":"2025-05-15T13:54:23.944504Z","shell.execute_reply":"2025-05-15T15:04:44.382749Z"}},"outputs":[{"name":"stdout","text":"Epoch: 01\n\tTrain Loss: 5.745\n\t Val. Loss: 5.329\nEpoch: 02\n\tTrain Loss: 5.261\n\t Val. Loss: 5.168\nEpoch: 03\n\tTrain Loss: 5.024\n\t Val. Loss: 5.104\nEpoch: 04\n\tTrain Loss: 4.853\n\t Val. Loss: 5.109\nEpoch: 05\n\tTrain Loss: 4.722\n\t Val. Loss: 5.088\nEpoch: 06\n\tTrain Loss: 4.605\n\t Val. Loss: 5.112\nEpoch: 07\n\tTrain Loss: 4.505\n\t Val. Loss: 5.069\nEpoch: 08\n\tTrain Loss: 4.417\n\t Val. Loss: 5.164\nEpoch: 09\n\tTrain Loss: 4.331\n\t Val. Loss: 5.166\nEpoch: 10\n\tTrain Loss: 4.264\n\t Val. Loss: 5.169\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"def translate_sentence(sentence, model, max_length=50):\n    model.eval()\n    \n    # Tokenize with preprocessing\n    tokens = simple_arabic_tokenizer(sentence) +['<eos>']\n    \n    # Convert to indices with unknown handling\n    src_indices = [ar_word_to_idx.get(token, ar_word_to_idx['<unk>']) for token in tokens]\n    \n    # Create tensor and move to device (add <sos> and <eos>)\n    src_indices = src_indices \n    src_tensor = torch.LongTensor(src_indices).unsqueeze(1).to(device)  # [seq_len, 1]\n    \n    # Encoder forward pass\n    with torch.no_grad():\n        embedded = model.dropout(model.encoder_embedding(src_tensor))\n        encoder_outputs, (hidden, cell) = model.encoder(embedded)\n        \n        # Reshape hidden states for decoder\n        hidden = model._reshape_hidden(hidden)\n        cell = model._reshape_hidden(cell)\n    \n    # Initialize target with <sos>\n    trg_indices = [en_word_to_idx['<sos>']]\n    \n    # Decoding loop with attention\n    for _ in range(max_length):\n        trg_tensor = torch.LongTensor([trg_indices[-1]]).to(device)  # [1]\n        \n        with torch.no_grad():\n            # Get decoder embeddings\n            embedded = model.dropout(model.decoder_embedding(trg_tensor.unsqueeze(0)))\n            \n            # Calculate attention context\n            context, _ = model.attention(hidden[-1], encoder_outputs)\n            context = context.unsqueeze(0)\n            # Combine embedded input with context\n            rnn_input = torch.cat((embedded, context), dim=2)\n            \n            # Decoder step\n            output, (hidden, cell) = model.decoder(rnn_input, (hidden, cell))\n            \n            # Combine output with context\n            output = torch.cat((output, context), dim=2)\n            \n            # Predict next token\n            prediction = model.fc_out(output.squeeze(0))\n        \n        # Get predicted token\n        pred_token = prediction.argmax(1).item()\n        trg_indices.append(pred_token)\n        \n        # Stop if <eos> is generated\n        if pred_token == en_word_to_idx['<eos>']:\n            break\n    \n    # Convert indices to tokens\n    trg_tokens = [en_idx_to_word[i] for i in trg_indices]\n    \n    # Join tokens into sentence (use moses detokenizer for better results)\n    return ' '.join(trg_tokens[1:-1])  # Remove <sos> and <eos>","metadata":{"id":"QDLzcmz5zqMt","trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:04:44.384952Z","iopub.execute_input":"2025-05-15T15:04:44.385275Z","iopub.status.idle":"2025-05-15T15:04:44.393479Z","shell.execute_reply.started":"2025-05-15T15:04:44.385247Z","shell.execute_reply":"2025-05-15T15:04:44.392719Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# Load the best model\nmodel.load_state_dict(torch.load('best_model_small.pt'))\n\n# Test on some sentences\ntest_sentences = [\n    \"أخرج من هنا الأن\",\n    \"أنا أحب التعلم الآلي\",\n    \"ما هو اسمك\",\n    \"هل يمكنك مساعدتي\",\n    \"شكرا لك\"\n]\n\nfor sent in test_sentences:\n    translation = translate_sentence(sent, model)\n    print(f\"Arabic: {sent}\")\n    print(f\"English: {translation}\")\n    print()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Th5LSO6k5BOK","outputId":"c97ca1a7-8e35-4228-fe5d-84b7772e09ef","trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:04:44.394170Z","iopub.execute_input":"2025-05-15T15:04:44.394529Z","iopub.status.idle":"2025-05-15T15:04:44.595329Z","shell.execute_reply.started":"2025-05-15T15:04:44.394501Z","shell.execute_reply":"2025-05-15T15:04:44.594475Z"}},"outputs":[{"name":"stdout","text":"Arabic: أخرج من هنا الأن\nEnglish: out of here now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now\n\nArabic: أنا أحب التعلم الآلي\nEnglish: i love like a doctor\n\nArabic: ما هو اسمك\nEnglish: what is he matter\n\nArabic: هل يمكنك مساعدتي\nEnglish: can you help me\n\nArabic: شكرا لك\nEnglish: thank you thank you thank you thank you thank you thank you thank you thank you thank you thank you thank you thank you thank you thank you thank you thank you thank you thank you thank you thank you thank you thank you thank you thank you thank\n\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"def evaluate_test_set(model, test_loader, max_length=50):\n    model.eval()\n    total = 0\n    correct = 0\n    all_references = []\n    all_hypotheses = []\n    smoothing = SmoothingFunction()\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            src = batch['arabic'].to(device)\n            trg = batch['english'].to(device)\n            \n            # Get batch translations\n            batch_translations = []\n            for i in range(src.size(1)):\n                # Convert tensor to tokens\n                src_tokens = [ar_idx_to_word[idx.item()] for idx in src[:,i] if idx.item() != ar_word_to_idx['<pad>']]\n                src_sentence = ' '.join(src_tokens[1:-1])  # Remove <sos> and <eos>\n                \n                # Get translation\n                if not src_sentence.strip():\n                    continue\n                translation = translate_sentence(src_sentence, model, max_length)\n                batch_translations.append(translation)\n                \n                # Get reference\n                ref_tokens = [en_idx_to_word[idx.item()] for idx in trg[:,i] if idx.item() != en_word_to_idx['<pad>']]\n                reference = ' '.join(ref_tokens[1:-1])\n                \n                \n                # For word-level accuracy\n                pred_words = set(translation.split())\n                true_words = set(reference.split())\n                correct += len(pred_words & true_words)\n                total += len(true_words)\n    \n    # Calculate metrics\n    word_accuracy = correct / total if total > 0 else 0\n    \n   \n    print(\"\\nEvaluation Results:\")\n    print(f\"test Accuracy: {word_accuracy:.2%}\")\n    print(\"\\nSample Translations:\")\n    for i, (ref, hyp) in enumerate(zip(\n        [' '.join(ref[0]) for ref in all_references[:5]],\n        [' '.join(hyp) for hyp in all_hypotheses[:5]]\n    )):\n        print(f\"\\nSample {i+1}:\")\n        print(f\"Reference: {ref}\")\n        print(f\"Generated: {hyp}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:04:44.597117Z","iopub.execute_input":"2025-05-15T15:04:44.597328Z","iopub.status.idle":"2025-05-15T15:04:44.605423Z","shell.execute_reply.started":"2025-05-15T15:04:44.597312Z","shell.execute_reply":"2025-05-15T15:04:44.604620Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"test_metrics = evaluate_test_set(model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:04:44.606404Z","iopub.execute_input":"2025-05-15T15:04:44.607062Z","iopub.status.idle":"2025-05-15T15:05:19.509926Z","shell.execute_reply.started":"2025-05-15T15:04:44.607009Z","shell.execute_reply":"2025-05-15T15:05:19.509336Z"}},"outputs":[{"name":"stdout","text":"\nEvaluation Results:\ntest Accuracy: 8.53%\n\nSample Translations:\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\nimport numpy as np\n\ndef calculate_bleu(dataset, model, max_length=50, num_samples=None):\n    \"\"\"\n    Calculate BLEU score for model translations\n    \n    Args:\n        dataset: Dataset containing source and reference translations\n        model: Your Seq2SeqWithAttention model\n        max_length: Maximum length for generated translations\n        num_samples: Number of samples to evaluate (None for all)\n    \"\"\"\n    references = []\n    hypotheses = []\n    smoothie = SmoothingFunction().method4  # Smoothing for short sentences\n    \n    if num_samples is None:\n        num_samples = len(dataset)\n    \n    for i in range(min(num_samples, len(dataset))):\n        # Get source and reference\n        item = dataset[i]['translation']\n        src_text = item['ar']\n        ref_text = item['en']\n        \n        # Tokenize reference\n        ref_tokens = simple_english_tokenizer(ref_text)\n        references.append([ref_tokens])  # Note: wrapped in list for multiple references\n        \n        # Generate translation\n        hyp_text = translate_sentence(src_text, model, max_length)\n        hyp_tokens = simple_english_tokenizer(hyp_text)\n        hypotheses.append(hyp_tokens)\n        \n        # Print sample translations\n        if i < 3:  # Print first 3 examples\n            print(f\"\\nSample {i+1}:\")\n            print(f\"Arabic:    {src_text}\")\n            print(f\"Reference: {ref_text}\")\n            print(f\"Generated: {hyp_text}\")\n    \n    # Calculate BLEU scores\n    bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n    bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)\n    bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n    \n    print(\"\\nBLEU Scores:\")\n    print(f\"BLEU-1: {bleu1*100:.2f}\")\n    print(f\"BLEU-2: {bleu2*100:.2f}\")\n    print(f\"BLEU-4: {bleu4*100:.2f}\")\n    \n    return {\"bleu1\": bleu1, \"bleu2\": bleu2, \"bleu4\": bleu4}","metadata":{"id":"KRgAYfhOJZRZ","trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:05:19.510645Z","iopub.execute_input":"2025-05-15T15:05:19.510859Z","iopub.status.idle":"2025-05-15T15:05:19.518278Z","shell.execute_reply.started":"2025-05-15T15:05:19.510845Z","shell.execute_reply":"2025-05-15T15:05:19.517416Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# Calculate BLEU on validation set (first 100 samples for quick evaluation)\nbleu_scores = calculate_bleu(test_data, model, num_samples=100)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:05:19.519136Z","iopub.execute_input":"2025-05-15T15:05:19.519408Z","iopub.status.idle":"2025-05-15T15:05:21.668533Z","shell.execute_reply.started":"2025-05-15T15:05:19.519385Z","shell.execute_reply":"2025-05-15T15:05:21.667761Z"}},"outputs":[{"name":"stdout","text":"\nSample 1:\nArabic:    أجل.\nReference: Yeah.\nGenerated: \n\nSample 2:\nArabic:    وإذا لم تكن هى؟\nReference: What if it's not her?\nGenerated: if if not not not yet\n\nSample 3:\nArabic:    وإن ضمان الأمن ومصداقيته في كابل وغيرها من المراكز لهو ذو أهمية قصوى.\nReference: Ensuring credible security in Kabul and other centres is of paramount importance.\nGenerated: security security security security security security security security security and the <unk> of of the <unk> of of the\n\nBLEU Scores:\nBLEU-1: 12.49\nBLEU-2: 5.97\nBLEU-4: 1.07\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}